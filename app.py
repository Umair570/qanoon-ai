import os
import sys
import json
import time
import threading
from flask import Flask, render_template, request, jsonify, Response, stream_with_context
from dotenv import load_dotenv

from langchain_groq import ChatGroq

sys.path.append(os.getcwd()) 
load_dotenv()  

groq_api_key = os.getenv("GROQ_API_KEY")
if not groq_api_key:
    print("âŒ ERROR: GROQ_API_KEY not found in .env file.")

try:
    llm = ChatGroq(
        temperature=0.3, 
        model_name="llama-3.1-8b-instant", 
        api_key=groq_api_key,
        max_tokens=300, # ğŸ’¥ Physically stops the AI from generating endless paragraphs
        model_kwargs={
            "frequency_penalty": 1.0, # ğŸ’¥ Mathematically blocks the AI from repeating sentences
            "presence_penalty": 0.5   # ğŸ’¥ Encourages the AI to introduce new concepts rather than looping
        }
    )
    print("âš¡ SUCCESS: Groq AI Model Ready!")
except Exception as e:
    print(f"âŒ ERROR: Groq Initialization Failed - {e}")

print("ğŸ”Œ Initializing Cloud Brain on Startup...")
rag = None
try:
    from backend.ai.rag_engine import RAGEngine
    rag = RAGEngine()
    
    print("ğŸ”¥ Forcing Hugging Face API to wake up (This may take a minute)...")
    is_awake = False
    while not is_awake:
        try:
            rag.embeddings.embed_query("wake up")
            is_awake = True
            print("âœ… SUCCESS: Hugging Face API is fully awake and ready!")
        except Exception:
            print("â³ Hugging Face is still booting. Knocking again in 5 seconds...")
            time.sleep(5)
            
except Exception as e:
    print(f"âŒ ERROR: Cloud AI Memory Failed - {e}")

def keep_brain_awake():
    while True:
        time.sleep(300) 
        if rag:
            try:
                rag.embeddings.embed_query("heartbeat ping")
                print("ğŸ’“ [Heartbeat] Sent signal to keep Hugging Face awake.")
            except Exception:
                pass 

threading.Thread(target=keep_brain_awake, daemon=True).start()

app = Flask(__name__)

def generate_groq_response(prompt, max_retries=3):
    for attempt in range(max_retries):
        try:
            for chunk in llm.stream(prompt):
                if chunk.content:
                    yield chunk.content
            return 

        except Exception as e:
            error_msg = str(e).lower()
            if '429' in error_msg or 'rate_limit' in error_msg:
                wait_time = (attempt + 1) * 5 
                time.sleep(wait_time)
            else:
                yield f"âš ï¸ API Error: {str(e)}"
                return

    yield (
        "<h3>âš ï¸ Daily Limit Reached</h3>"
        "Qanoon AI has reached its maximum server capacity today. Please try again tomorrow!"
    )

@app.route('/')
def home(): return render_template('index.html')

@app.route('/consult', methods=['POST'])
def consult():
    data = request.json
    user_text = data.get('text', '')
    language_mode = data.get('lang', 'en') 
    
    print(f"ğŸ” Analyzing ({language_mode}): {user_text}")
    
    context = "No specific legal document found."
    
    if rag:
        try:
            docs = rag.search(user_text, k=5)
            if docs:
                context = ""
                for doc in docs:
                    context += f"\n--- SOURCE: {doc['title']} ---\n{doc['text']}\n"
        except Exception as e:
            def generic_error_message():
                yield f"<h3>âš ï¸ Memory Search Error</h3>An error occurred while searching the database: {str(e)}"
            return Response(stream_with_context(generic_error_message()), mimetype='text/plain')

    # --- COMPLETELY SPLIT NATIVE PROMPTS TO PREVENT AI CONFUSION ---
    if language_mode == 'ur':
        # Pure Urdu Instructions for Llama-3
        system_prompt = (
            "Ø¢Ù¾ 'Ù‚Ø§Ù†ÙˆÙ† Ø§Û’ Ø¢Ø¦ÛŒ' ÛÛŒÚºØŒ Ø¬Ùˆ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©Û’ Ù‚Ø§Ù†ÙˆÙ† Ú©Ø§ Ù…Ø§ÛØ± Ø§ÙˆØ± Ù…Ø´ÛŒØ± ÛÛ’Û”\n"
            "Ø¢Ù¾ Ú©Ùˆ ØµØ±Ù Ø§ÙˆØ± ØµØ±Ù ÙØ±Ø§ÛÙ… Ú©Ø±Ø¯Û 'DATA' Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ Ù¾Ø± Ø¬ÙˆØ§Ø¨ Ø¯ÛŒÙ†Ø§ ÛÛ’Û”\n\n"
            "ğŸš¨ Ø§ÛÙ… Ù‚ÙˆØ§Ù†ÛŒÙ†:\n"
            "1. Ø§Ú¯Ø± Ø¬ÙˆØ§Ø¨ DATA Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛÛŒÚº ÛÛ’ØŒ ØªÙˆ Ø¨Ø§Ù„Ú©Ù„ ÛŒÛ Ù„Ú©Ú¾ÛŒÚº: 'ğŸ›‘ [REJECTED] Ù…Ø¹Ø°Ø±ØªØŒ Ù…ÛŒØ±Û’ Ù¾Ø§Ø³ Ø§Ø³ Ú©Û’ Ø¨Ø§Ø±Û’ Ù…ÛŒÚº Ù…Ø®ØµÙˆØµ Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù†ÛÛŒÚº ÛÛŒÚºÛ”'\n"
            "2. Ø§Ú¯Ø± Ø³ÙˆØ§Ù„ Ù‚Ø§Ù†ÙˆÙ† Ø³Û’ Ù…ØªØ¹Ù„Ù‚ Ù†ÛÛŒÚº ÛÛ’ ÛŒØ§ ØºÛŒØ± Ø§Ø®Ù„Ø§Ù‚ÛŒ ÛÛ’ ØªÙˆ Ù„Ú©Ú¾ÛŒÚº: 'ğŸ›‘ [REJECTED] Ù…ÛŒÚº ØµØ±Ù Ù¾Ø§Ú©Ø³ØªØ§Ù†ÛŒ Ù‚Ø§Ù†ÙˆÙ† Ø³Û’ Ù…ØªØ¹Ù„Ù‚ Ø³ÙˆØ§Ù„Ø§Øª Ú©Û’ Ø¬ÙˆØ§Ø¨Ø§Øª Ø¯Û’ Ø³Ú©ØªØ§ ÛÙˆÚºÛ”'\n\n"
            "ğŸ’¬ Ø¬ÙˆØ§Ø¨ Ú©Ø§ Ø·Ø±ÛŒÙ‚Û Ú©Ø§Ø±:\n"
            "- Ø¬ÙˆØ§Ø¨ Ø§Ù†ØªÛØ§Ø¦ÛŒ Ù…Ø®ØªØµØ± (Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û 3 ÛŒØ§ 4 Ø¬Ù…Ù„Û’) Ø§ÙˆØ± Ø¢Ø³Ø§Ù† Ø§Ø±Ø¯Ùˆ Ù…ÛŒÚº Ø¯ÛŒÚºÛ”\n"
            "- Ú©Ø³ÛŒ Ø¨Ú¾ÛŒ Ø¬Ù…Ù„Û’ ÛŒØ§ Ø¨Ø§Øª Ú©Ùˆ Ø¯ÙˆØ¨Ø§Ø±Û Ù…Øª Ø¯ÛØ±Ø§Ø¦ÛŒÚºÛ”\n"
            "- Ø³Ø²Ø§Ø¤Úº Ú©Ùˆ Ù†Ù…Ø§ÛŒØ§Úº Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ **Ù…ÙˆÙ¹Û’ Ø§Ù„ÙØ§Ø¸** (Bold text) Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚºÛ”\n"
            "- Ø¢Ø®Ø± Ù…ÛŒÚº Ù‚Ø§Ù†ÙˆÙ† Ú©Ø§ Ø­ÙˆØ§Ù„Û Ø§Ø³ Ø·Ø±Ø­ Ø¯ÛŒÚº: 'ğŸ“– Reference: Section [Number]'.\n"
        )
    else:
        # Pure English Instructions
        system_prompt = (
            "You are Qanoon AI, a professional, modern legal advisor for Pakistani law.\n"
            "You MUST answer strictly using the provided DATA.\n\n"
            "ğŸš¨ CRITICAL RULES:\n"
            "1. If the answer is not explicitly in the DATA, respond exactly with: 'ğŸ›‘ [REJECTED] I am sorry, but I do not have specific information regarding this in my current legal records.'\n"
            "2. If the query is unrelated to Pakistani law or offensive, respond exactly with: 'ğŸ›‘ [REJECTED] I am Qanoon AI, a professional legal assistant. I can only answer questions related to Pakistani law.'\n\n"
            "ğŸ’¬ FORMATTING:\n"
            "- Answer in a natural, conversational tone. Keep it very concise (max 3-4 sentences).\n"
            "- Use short bullet points ONLY if listing multiple penalties.\n"
            "- Bold the actual penalty, prison time, or fine amount.\n"
            "- NEVER repeat the same sentence twice.\n"
            "- End with a clean citation on a new line: 'ğŸ“– Reference: Section [Number]'.\n"
        )

    full_prompt = f"{system_prompt}\n\nDATA:\n{context}\n\nQUERY: {user_text}"

    # ğŸ’¥ RESTORED THE MISSING RETURN STATEMENT HERE ğŸ’¥
    return Response(stream_with_context(generate_groq_response(full_prompt)), mimetype='text/plain')

LAWYERS_DB_PATH = os.path.join("backend", "data", "raw", "lawyers_db.json")

@app.route('/lawyers', methods=['GET'])
def get_lawyers():
    all_lawyers = []
    filtered_lawyers = []
    category = request.args.get('category', 'general').lower().strip()
    
    try:
        if os.path.exists(LAWYERS_DB_PATH):
            with open(LAWYERS_DB_PATH, 'r', encoding='utf-8') as f:
                all_lawyers = json.load(f)
        else:
            return jsonify([]) 
    except Exception as e:
        return jsonify([])

    if not all_lawyers:
        return jsonify([])

    if category == 'general' or not category:
        return jsonify(all_lawyers[:10])
    
    for lawyer in all_lawyers:
        lawyer_tags = [t.lower() for t in lawyer.get('tags', [])]
        lawyer_specialty = lawyer.get('specialty', '').lower()
        if category in lawyer_tags or category in lawyer_specialty:
            filtered_lawyers.append(lawyer)
    
    if not filtered_lawyers:
        return jsonify(all_lawyers[:5])
        
    return jsonify(filtered_lawyers)

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=int(os.environ.get("PORT", 5000)))